---
layout: default
title: 5주차 WIL
---

## 5주차 WIL: 한 단계 더 성장한 LLM 활용법

이번 5주차는 단순한 LLM 사용이 아니라, LLM을 더 똑똑하게 만드는 여러 가지 기술을 학습하는 주차였다.  
Instruction Tuning, Multi-modal LLM(MLLM), RAG(Retrieval-Augmented Generation) 세 가지를 배우면서,  
각 개념이 실제 서비스에 어떻게 적용될 수 있는지를 고민하는 시간이 되었다.  
  
특히 이번 주는 단순히 개념만 이해하는 것이 아니라,  
LangChain을 활용해 코드를 짜보고 다양한 Knowledge Source를 연결하며 직접 실습했기 때문에  
훨씬 더 실질적인 경험이 되었다.  
  
5주차를 지나며, 나는 LLM을 활용하는 관점이 단순히 사용하는 것을 넘어서  
어떻게 최적화된 지능형 시스템을 설계할 수 있을지 고민하는 방향으로 바뀌고 있음을 느꼈다.  
  
---

### Instruction Tuning  
  
LLM은 기본적으로 Next Token Prediction을 기반으로 학습된 모델이다.  
하지만 이 방식만으로는 사람의 의도를 정확히 파악하고 질문에 제대로 답하는 데 어려움이 있었다.  
  
Instruction Tuning은 이러한 한계를 극복하기 위해 등장했다.  
(Instruction, Output) 형태의 데이터셋을 활용해 모델을 미세 조정하면서,  
사용자의 지시에 따라 더 정확하게 답변할 수 있도록 훈련시키는 것이다.  
  
특히 Supervised Fine-Tuning(SFT) 과정에서는,  
모델이 답변만 생성하도록 Loss를 설계해야 한다는 점이 중요하게 다가왔다.  
그렇지 않으면 모델이 질문까지 같이 생성해버리는 문제가 발생할 수 있기 때문이다.  
  
Self-Instruct, GSM-Plus 같은 실제 사례를 통해,  
사람이 직접 만드는 데이터와 모델이 보조 생성하는 데이터 간 균형을 어떻게 맞출지 고민하게 되었다.  
  
좋은 Instruction 데이터셋이 결국 모델 품질을 좌우한다는 사실을 깊이 깨달았다.  
앞으로 나도 모델을 다룰 때 데이터 품질 설계에 훨씬 더 신경 써야겠다는 생각을 하게 됐다.  
  
---
  
### Multi-modal LLM (MLLM)  
  
세상은 텍스트만으로 이뤄지지 않는다.  
이미지, 오디오, 비디오 등 다양한 정보가 존재한다.  
  
이런 현실 세계를 제대로 이해하려면, AI 역시 다양한 형태의 데이터를 처리할 수 있어야 한다.  
이런 필요를 충족시키기 위해 등장한 것이 Multi-modal LLM, 줄여서 MLLM이다.  
  
MLLM은 ViT(Vision Transformer)를 이용해 이미지를 Token Sequence로 변환하고,  
이를 LLM에 자연스럽게 연결함으로써, 텍스트와 이미지를 함께 이해할 수 있도록 한다.  
  
특히 ViT가 이미지를 일정한 패치 단위로 나누고,  
이를 Transformer 입력처럼 처리하는 방식이 매우 흥미로웠다.  
  
실습에서는 직접 이미지를 불러와 GPT-4o-mini 모델에 입력하고,  
이미지 기반 질문에 답변을 생성하는 과정을 경험했다.  
  
MLLM은 단순히 새로운 기능을 추가하는 수준이 아니라,  
AI가 세상을 이해하는 방식 자체를 확장시키는 핵심 기술임을 느낄 수 있었다.  
  
---
  
### Retrieval-Augmented Generation (RAG)  
  
RAG는 이번 주의 학습에서 가장 중요한 주제 중 하나였다.  
  
기존 LLM은 훈련 데이터에 포함된 지식만을 기반으로 답변을 생성했다.  
이 방식은 최신 정보나 특정 도메인 지식에 대한 한계가 분명했다.  
  
RAG는 LLM에 외부 Knowledge Source를 연결해,  
모델이 필요한 정보를 검색하고, 이를 바탕으로 답변을 생성하도록 돕는다.  
  
실습에서는 LangChain, Chroma, OpenAI Embeddings를 활용해 RAG 시스템을 직접 구축했다.  
진행 과정은 다음과 같다.  
• 웹사이트에서 텍스트를 불러오고  
• Text Splitter로 적절하게 Chunking하고  
• Chroma에 임베딩하고  
• Retriever로 관련 문서를 검색한 후  
• 검색된 결과를 기반으로 GPT에게 답변을 요청하는 일련의 흐름을 만들었다.  
  
RAG를 통해 LLM의 성능을 본질적으로 향상시킬 수 있음을 체감할 수 있었다.  
LLM은 이제 더 이상 내부 지식에만 의존하지 않고, 외부에서 필요한 정보를 가져와 유연하게 답변할 수 있게 되었다.  
  
---

  
### 기본 과제
  
기본 과제에서는 Sparta Coding Club 블로그를 Knowledge Source로 활용해,  
“ALL-in 코딩 공모전 수상작들을 요약해줘.” 라는 질문에 답하는 RAG 시스템을 구축했다.  
  
진행 과정은 다음과 같았다.  
• WebBaseLoader로 블로그 글을 불러오고  
• 필요한 부분만 Parsing하도록 HTML Class를 지정하고  
• RecursiveCharacterTextSplitter로 문서를 Chunking한 뒤  
• Chroma에 저장하고  
• Retriever를 통해 문서를 검색해 GPT-4o-mini에 연결했다.  
  
최종적으로, 공모전 수상작들에 대해 명확하고 요약된 답변을 생성할 수 있었다.  
  
하지만 Chunking 과정에서 더 섬세하게 구간을 나누어야 한다는 점을 느꼈다.  
Chunk Size나 Overlap 설정이 결과물의 품질에 상당히 큰 영향을 미쳤다.  
  
또한, Knowledge Source의 품질이 RAG 전체 품질을 좌우한다는 사실을 다시 한번 깊이 깨달았다.  
  
---

  
### 심화 과제
  
심화 과제에서는 패션 추천 서비스를 기획하고 간단한 형태로 구현해보았다.  
  
주제 선정 이유는 다음과 같다.  
요즘 AI 패션 추천 서비스가 주목받고 있고,  
사용자의 키워드 입력을 바탕으로 맞춤형 추천이 가능하다면  
향후 고도화할 수 있는 잠재력이 있다고 생각했기 때문이다.  
  
진행 과정은 다음과 같다.  
• 패션 관련 문서와 데이터셋을 수집하고  
• 간단한 키워드 기반 매칭 로직을 통해 추천 시스템을 구현했다  
• 사용자가 입력한 키워드(예: 스타일, 계절, 분위기 등)에 따라  
• 사전에 정리된 데이터를 조회해 추천 문장을 생성했다  
  
예를 들어,  
사용자가 “여름에 입을 수 있는 시원한 캐주얼 스타일”이라고 입력하면,  
린넨 셔츠, 반팔 폴로 티셔츠 같은 아이템을 추천하는 식이다.  
  
현재는 복잡한 검색 시스템이나 RAG 구조는 적용하지 않았고,  
간단한 키워드 기반 매칭을 활용해 구현했다.  
  
하지만 이후 고도화 단계에서는 RAG나 벡터 검색 기반 추천 시스템을 도입해,  
더 세밀하고 맞춤형 추천을 제공할 수 있도록 발전시킬 계획이다.  
  
---

  
### 느낀 점  
  
이번 5주차를 통해,  
나는 LLM을 단순 사용하는 단계를 넘어,  
직접 시스템을 설계하고 최적화하는 방향으로 성장하고 있음을 느꼈다.  
  
특히  
• Instruction Tuning으로 LLM의 답변 스타일을 제어하고  
• MLLM으로 다양한 입력 타입을 다루고  
• RAG를 통해 외부 지식을 효과적으로 연결하는  
  
과정을 직접 실습하며, AI 시스템 구축의 실질적인 감을 익힐 수 있었다.  
  
앞으로는  
• Multi-turn 대화를 지원하는 고급형 RAG 시스템 설계  
• PDF, API, 영상 등 다양한 데이터 타입을 지원하는 Retrieval  
• 사용자 맞춤형 개인화 에이전트 구축  
  
에 도전하고 싶다.  
  
이번 경험을 발판 삼아, 나만의 AI 서비스를 만들 수 있도록 꾸준히 노력할 계획이다.  
  
  
#항해99 #항해플러스AI후기 #개발자커뮤니티 #LLM
